{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online data collection using the requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please inlcude your names below\n",
    "# Also, please edit the name of the file and include the names of the two(or three) people answering\n",
    "\n",
    "# Pair answering the assignment: Jialu Liu, Michael Blum\n",
    "# Pair giving feedback: ..., ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As step 0, pick your favorite Wikipedia page, open it in the browser, and then save it as an html file. Now open it in the browser as well as in a text editor and look at the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the requests library you can retrieve the html source of the page, in a response object (using requests.get(“url”)). The response object you received has content that you can access calling the .text function on it.\n",
    "\n",
    "Call text and save the result in a file, then open the file in a browser and check whether you successfully saved the page. Note, you will only be able to open the file in the browser if you give it an html extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Basic web crawling\n",
    "\n",
    "URLs have specific formats, for example any Wikipedia page will be of the format https://en.wikipedia.org/wiki/Pythonidae where the last word is the topic of the article.\n",
    "Next, we want to automate this saving process using the requests library and making automated requests to Wikipedia.\n",
    "\n",
    "Exercise: Pick 5 different words, and write code that loops through these words, and retrieves the html content for each associated wikipedia page, and saves the html text as wiki_htmls/[word].html files. (Choose words that actually have associated wiki pages). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n",
      "End\n",
      "End\n",
      "End\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "### your code here\n",
    "\n",
    "import requests\n",
    "\n",
    "def getHtml(url):\n",
    "    r=requests.get(url)\n",
    "    html=r.text\n",
    "    return html\n",
    "\n",
    "def saveHtml(file_name, file_content):\n",
    "    with open(file_name.replace('/','_')+'.html','wb') as f:\n",
    "        f.write(file_content)\n",
    "\n",
    "s = 'https://en.wikipedia.org/wiki/'\n",
    "a = ['Google', 'Baidu', 'Twitter', 'Github', 'QQ']\n",
    "for i in a:\n",
    "    web=s+i\n",
    "    html=str.encode(getHtml(web))\n",
    "    #file.write(html)\n",
    "    saveHtml(\"wiki_htmls/\"+'['+i+']',html)\n",
    "    print(\"End\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) URL formats\n",
    "\n",
    "What is the common URL in the case of Google searches? And in the case of Yelp? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Google searches:\n",
    "    https://www.google.com/search?\n",
    "Yelp:\n",
    "    https://de.yelp.ch/search?find_desc=&find_loc="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what happens to the URL if you want to define the location as well as the type of venue you are looking for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp:\n",
    "    https://de.yelp.ch/search?find_desc=[type of venue]&find_loc=[location]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find more search parameters for either of the two pages that you can define via the URLs? What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Google search:\n",
    "    q:Query(key words)\n",
    "    ie:Input encoding\n",
    "    hl:Interface language\n",
    "    sa:Safe search setting\n",
    "    start:Page number of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) And now let's work with the HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BeautifulSoup parser library we will parse the webpage that you just saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import BeautifulSoup, our parser library\n",
    "# And make a soup object out of the html of the page\n",
    "\n",
    "# in case bs4 throws error try\n",
    "# !pip install --upgrade html5lib==1.0b8\n",
    "\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ModuleNotFoundError:\n",
    "    !pip3 install --upgrade html5lib==1.0b8\n",
    "    !pip3 install BeautifulSoup4\n",
    "    from BeautifulSoup4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   A simple example page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div>\n",
      "   <p class=\"inner-text first-item\" id=\"first\">\n",
      "    First paragraph.\n",
      "   </p>\n",
      "   <p class=\"inner-text\">\n",
      "    Second paragraph.\n",
      "   </p>\n",
      "  </div>\n",
      "  <p class=\"outer-text first-item\" id=\"second\">\n",
      "   <b>\n",
      "    First outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"outer-text\">\n",
      "   <b>\n",
      "    Second outer paragraph.\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# print a nice version using prettify\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can find all instances of a tag at once: Try to predict what the following command will return: `soup.find_all('p')` and then call it to check if you were right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"inner-text first-item\" id=\"first\">\n",
      "                First paragraph.\n",
      "            </p> \n",
      "\n",
      "<p class=\"inner-text\">\n",
      "                Second paragraph.\n",
      "            </p> \n",
      "\n",
      "<p class=\"outer-text first-item\" id=\"second\">\n",
      "<b>\n",
      "                First outer paragraph.\n",
      "            </b>\n",
      "</p> \n",
      "\n",
      "<p class=\"outer-text\">\n",
      "<b>\n",
      "                Second outer paragraph.\n",
      "            </b>\n",
      "</p> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#it will return all tags containing the letter \"p\"\n",
    "find_all_p = soup.find_all('p')\n",
    "for line in find_all_p:\n",
    "    print(line, '\\n')\n",
    "# assumption was right; we got all paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the second element of this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"inner-text\">\n",
      "                Second paragraph.\n",
      "            </p>\n"
     ]
    }
   ],
   "source": [
    "list1=soup.find_all('p')\n",
    "print(list1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the text inside the second element of the list, using the .text on the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                Second paragraph.\\n            '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you try to find a specific element on a page you can reach it by finding classes or IDs of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How many elements would it return for 'inner_class'? Guess, and check your guess by using the find_all command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# guess: there are 2 paragraphs with the class name 'inner-text'\n",
    "list2=soup.find_all('p', class_='inner-text')\n",
    "print(len(list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4) Finding elements in the browser\n",
    "Since every web page is different and html can get very large and messy, the easiest way to find elements that you are interested in is to start from the browser window. So next we will quickly look at how to find elements using the developer tools in your browser. Open the following webpage in your browser (preferably Chrome): http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579#.Wkwh8VQ-fVo \n",
    "\n",
    "Find the developer tools in your browser. (In Chrome, it's view --> developer --> developer tools or Control+Shift+C on Windows and Command+Shift+C on Mac) You should end up with a panel at the bottom or the right side of the browser like what you see below. Make sure the Elements panel is highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to find a specific element, you can right click on it on the page and select \"inspect\". This will also open up the developer tools window. For example if we want to extract the current temperature value:\n",
    "\n",
    "<img src=\"inspect.png\">\n",
    "\n",
    "<img src=\"inspect_class.png\">\n",
    "\n",
    "<br><br>\n",
    "1. Using the find function, extract and print out the current temperature from the page. \n",
    "2. Do the same with the value in Celsius. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The temperature in Fahrenheit is 70°F\n",
      "The temperature in Celsius is 21°C\n"
     ]
    }
   ],
   "source": [
    "### Fill out and print a full sentence describing the temperature in F and C. \n",
    "\n",
    "temp_F = soup.find_all(class_='myforecast-current-lrg')[0].text\n",
    "temp_C = soup.find_all(class_='myforecast-current-sm')[0].text\n",
    "print(\"The temperature in Fahrenheit is \" + temp_F)\n",
    "print(\"The temperature in Celsius is \" + temp_C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In this exercise we will extract each day's forecast from the 7 day extended forecast on the weather report page. <br>\n",
    "    a. Find the container for the seven day forecast on the weather page we just downloaded. <br>\n",
    "    b. Make a list with all forecast items (overnight, Wednesday, Wednesday night, etc) <br>\n",
    "    c. For each time period, print out the name of the period, the short description of the expected weather conditions, and the temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overnight the weather will be showers likely and breezy with a temperature of 69 °F.\n",
      "Friday the weather will be showers likely and breezy with a temperature of 81 °F.\n",
      "Friday Night the weather will be heavy rain and breezy with a temperature of 68 °F.\n",
      "Saturday the weather will be heavy rain with a temperature of 81 °F.\n",
      "Saturday Night the weather will be heavy rain with a temperature of 68 °F.\n",
      "Sunday the weather will be heavy rain with a temperature of 82 °F.\n",
      "Sunday Night the weather will be scattered showers with a temperature of 68 °F.\n",
      "Monday the weather will be scattered showers then sunny with a temperature of 82 °F.\n",
      "Monday Night the weather will be scattered showers with a temperature of 68 °F.\n"
     ]
    }
   ],
   "source": [
    "# For each time period print out something like: \n",
    "# Overnight the weather will be mostly clear and breezy and the temperature will be 65F.\n",
    "\n",
    "tombstones = soup.select('#seven-day-forecast-list > li > .tombstone-container')\n",
    "for tombstone in tombstones:\n",
    "    date = tombstone.select('.period-name')[0].get_text(separator=\" \")\n",
    "    short_desc = tombstone.select('.short-desc')[0].get_text(separator=\" \").replace('   ', ' ').lower()\n",
    "    temp = tombstone.select('.temp')[0].get_text()[-5:]\n",
    "    print(date + ' the weather will be ' + short_desc + ' with a temperature of ' + temp + '.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Take a list of jobs (e.g.['teacher', 'lawyer', 'data-scientist']). For each job save the html of the result of searching it on indeed. The url of a result page looks like: https://www.indeed.com/q-data-scientist-jobs.html. \n",
    "<br>\n",
    "For each job find the names of the companies from the first result page.  Make a dictionary where the keys are the jobs and value is a list of the company names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artist': ['Adriyana Solutions',\n",
      "            'Matthews International Corporation',\n",
      "            'On Point Marketing',\n",
      "            'Spartina 449',\n",
      "            'Filmless',\n",
      "            'Player One Trailers',\n",
      "            'MGA Entertainment Inc',\n",
      "            'Matthews International Corporation',\n",
      "            'Rooster Teeth',\n",
      "            'Transparent Language'],\n",
      " 'engineer': ['thyssenkrupp Industrial Solutions (USA)',\n",
      "              'Danville Metal Stamping Co',\n",
      "              'ASM NEXX',\n",
      "              'Henkel',\n",
      "              'ANSYS',\n",
      "              'Formlabs',\n",
      "              'Amazon.com Services LLC',\n",
      "              'Phoenix Contact',\n",
      "              'Volvo Group',\n",
      "              'LOCKHEED MARTIN CORPORATION'],\n",
      " 'entertainer': ['Free Agency',\n",
      "                 'Free Agency',\n",
      "                 'Universal Orlando',\n",
      "                 'Guest Services, Inc.',\n",
      "                 'Utah Jive',\n",
      "                 'American Cruise Lines',\n",
      "                 'Salt Lake Community College',\n",
      "                 'Free Agency',\n",
      "                 'FatCats Entertainment',\n",
      "                 'Free Agency']}\n"
     ]
    }
   ],
   "source": [
    "jobs = ['engineer', 'artist', 'entertainer']\n",
    "job_dict = {}\n",
    "for job in jobs:\n",
    "    job_dict[job] = []\n",
    "    url = 'https://www.indeed.com/q-' + job + '-jobs.html'\n",
    "    \n",
    "    html = str.encode(getHtml(url))\n",
    "    saveHtml(\"indeed/\"+'['+job+']', html)\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    job_results = soup.select('td#resultsCol [data-tn-component=\"organicJob\"]')\n",
    "    for job_result in job_results:\n",
    "        company = job_result.select('.company')[0].text.replace('\\n','')\n",
    "        job_dict[job].append(company)\n",
    "\n",
    "pprint(job_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/viewjob?jk=3ac71fb15693d7a9&fccid=e00e3209f0319eac&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=59b0db57aa40e2b1&fccid=e00e3209f0319eac&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=321b5d1e64863e94&fccid=8a2e5e25a9623039&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=ed5b3050c7ae6fbd&fccid=cb2a33b61768d00f&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=d3437b0d2a6d4bb3&fccid=a48ec0707748f15e&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=c149ff42e1911559&fccid=13ab1df0327a097a&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=542ac3c576d1de55&fccid=dd616958bd9ddc12&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=6233fef6785eb7ae&fccid=e00e3209f0319eac&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=a2e2826e81cd63b7&fccid=b9b8b6021bbe0971&vjs=3\n",
      "https://www.indeed.com/viewjob?jk=b450cdcbdcb7344b&fccid=e00e3209f0319eac&vjs=3\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.indeed.com/q-entertainer-jobs.html'\n",
    "   \n",
    "html = str.encode(getHtml(url))\n",
    "saveHtml(\"indeed/[entertainer]\", html)\n",
    "    \n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "job_results = soup.select('td#resultsCol [data-tn-component=\"organicJob\"]')\n",
    "\n",
    "index = 0\n",
    "for job_result in job_results:\n",
    "    href = job_result.select('.title > a')[0].get('href')\n",
    "    redirect_url = 'https://www.indeed.com/viewjob?' + href[8:]\n",
    "    print(redirect_url)\n",
    "    \n",
    "    html = str.encode(getHtml(redirect_url))\n",
    "    file_name = 'redirectUrls/Entertainer[' + str(index) + ']'\n",
    "    saveHtml(file_name, html)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Next, we will do a 2 step crawling exercise. First, request the page for one chosen job category. Then make a list of the links to all specific job ads on that page. In a second step, crawl and save the content to all of these links. Name the folders and files in a meaningful way that helps you identify them later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every request you send has a so called HTTP header (unrelated to the content of the message), for example to communicate the size of the message, the browser from which the request is coming from, or what kind of response it is expecting back in the response. \n",
    "\n",
    "1) Read up on this: What parts does a request contain exactly and what is the purpose of a header? \n",
    "\n",
    "2) Look in the browser: Take a URL and find the request header using the developer tools in your browser. (Hint: you will need to look inside 'network'). \n",
    "\n",
    "3) If you don’t tell python otherwise, it will use a default header when sending requests. What is this default when you use the requests library?\n",
    "\n",
    "4) The requests library allows to specify the headers of your request exactly. Set the header of your request (for the  URL you previously picked) to be the one copied from your browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your chosen URL: ##\n",
    "\n",
    "Default header of Python requests: ##\n",
    "\n",
    "Header in your browser: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now compare the response headers for the same URL in the browser, and by calling a function on the response object in your code. What differences do you see? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response header in your browser: ##\n",
    "\n",
    "Response header in the response in python: ##\n",
    "\n",
    "Difference: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'python-requests/2.21.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://de.wikipedia.org/wiki/Sherlock_(Fernsehserie)'\n",
    "req = requests.get(url)\n",
    "sent_header = req.request.headers\n",
    "\n",
    "# 1)  Purpose of the header is to give some meta-information about the request. For example\n",
    "#     GET or POST indicating if you just wanna get data or also write data.\n",
    "#     Most importantly it contains the URL and the status response (200 if ok)\n",
    "#     Many more fields can be set of course\n",
    "#     \n",
    "# 2)  url: https://twitter.com/home\n",
    "#     Referrer Policy: no-referrer-when-downgrade\n",
    "# \n",
    "# 3)  \n",
    "print(sent_header)\n",
    "\n",
    "# 4)\n",
    "url = 'https://twitter.com/home'\n",
    "headers = {\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1' \n",
    "          }\n",
    "\n",
    "r = requests.get(url, headers = headers)\n",
    "\n",
    "# 5)\n",
    "# Difference: many more header fields in the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for completing the first notebook! Now it’s time for feedback.\n",
    "1.\tPass your solution to the other pair in your group.\n",
    "2.\tInclude your feedback in the other pair’s notebook. Don’t forget to add your names at the top.\n",
    "3.\tReturn the notebook with feedback to the original pairs.\n",
    "4.\tUpload your notebook, with the feedback included by the other pair on OLAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of/suggest (among other things)\n",
    " - improvements in the code (e.g. readability, efficiency)\n",
    " - improvements in the answers (e.g. are they easy to understand, are they correct, how can they be improved?)\n",
    " - point out differences (e.g. are there any differences between the responses of the two pairs? if yes what are they, what is the cause, and in which way can they be useful?)\n",
    " \n",
    "In this specific notebook the questions to focus on for feedback are: 1, 2, 4 and 5. 3 was just an intro to parsing so no need to analyze in detail. Not all suggestions about the type of feedback apply to all types of questions, try to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below there is space for giving feedback. This space should be used only by the other pair in your group.\n",
    "\n",
    "'''\n",
    "Feedback here\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
